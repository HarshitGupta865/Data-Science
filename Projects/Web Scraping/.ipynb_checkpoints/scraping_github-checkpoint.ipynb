{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0245d7",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories For Trending Github Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eeb1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f51f54",
   "metadata": {},
   "source": [
    "## Project Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918dcfa",
   "metadata": {},
   "source": [
    "- In this project we are going to scrape https://github.com/topics\n",
    "- We are going to extract the trending github topics and for each topic we will extract the topic title, topic description and topic URL\n",
    "- For each topic we will extract 20 top repositories \n",
    "- For each repositories, we'll grab the repository name, its username, no. of stars the repository received, and the  repository URL\n",
    "- For each topic we'll create a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6665827",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = ' https://github.com'\n",
    "main_url = 'https://github.com/topics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c886d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main objective of this get_all_topics function is to parse through all the pages of github_topics and return 3 things\n",
    "1. Topic_titles\n",
    "2. Topic_description\n",
    "3. Topic_urls\n",
    "'''\n",
    "\n",
    "def get_all_topics(main_url):\n",
    "    \n",
    "    # There are a total of 6 github-topics pages. Hence variable i is used to parse all 6 pages.\n",
    "    i = 1\n",
    "    \n",
    "    topic_titles = []\n",
    "    topic_description = []\n",
    "    topic_urls = []\n",
    "    \n",
    "    for i in range(1,7):\n",
    "        filters = {'page': i}\n",
    "        \n",
    "        # Sending the GET request to github-topics webpage\n",
    "        response = requests.get(main_url, params=filters)\n",
    "        \n",
    "        # If GET request not made successfully then terminate the code else continue\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f'Content did not get loaded for page no. {i}')\n",
    "        \n",
    "        # Doc has the content of the page from which we will extract information\n",
    "        \n",
    "        doc = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        '''\n",
    "        Now we need to extract 3 things here. Topic title, Topic description and Topic url.\n",
    "        '''\n",
    "        \n",
    "        # To extract the title of each github topic\n",
    "        topic_title_tags = doc.find_all('p', {'class': 'f3 lh-condensed mb-0 mt-1 Link--primary'})\n",
    "        \n",
    "        # Extracting the text of topic title and appending it in the topic_titles list created.\n",
    "        for tag in topic_title_tags:\n",
    "            topic_titles.append(tag.text)\n",
    "        \n",
    "        # To extract the description of each github topic\n",
    "        topic_desc_tags = doc.find_all('p', {'class': 'f5 color-fg-muted mb-0 mt-1'})\n",
    "\n",
    "        # Extracting the text of topic description and appending it in the topic_description list created.\n",
    "        for tag in topic_desc_tags:\n",
    "            topic_description.append(tag.text.strip())\n",
    "        \n",
    "        # To extract the link of each github topic\n",
    "        topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "        \n",
    "        # Extracting the link of each topic and appending it in the topic_urls list created.\n",
    "        for tag in topic_link_tags:\n",
    "            topic_urls.append(base_url + tag['href'])\n",
    "            \n",
    "    return topic_titles, topic_description, topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599f39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the get_all_topics function\n",
    "topic_titles, topic_description, topic_urls = get_all_topics(main_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11aef5b",
   "metadata": {},
   "source": [
    "### Creating a CSV file to store all three of the github-topics information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61111e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binding the 3 lists on topics into a dictionary\n",
    "all_topics_dict = {'title': topic_titles, 'description': topic_description, 'url': topic_urls}\n",
    "\n",
    "# Converting the dictionary into a dataframe\n",
    "all_topics_df = pd.DataFrame(all_topics_dict)\n",
    "\n",
    "all_topics_df.to_csv('all_topics.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "486bd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(topic_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf595aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c53544",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_tags = repo.find_all('h3', {'class': 'f3 color-fg-muted text-normal lh-condensed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63d72694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9438cb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mrdoob/three.js\n"
     ]
    }
   ],
   "source": [
    "print(repo_tags[0].find_all('a')[1]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9d024aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_tags = repo.find_all('span', {'class': 'Counter js-social-count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10415c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'90.6k'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_tags[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "178b9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_star_num(str):\n",
    "    if str[-1] == 'k':\n",
    "        return int(float(str[:-1])*1000)\n",
    "    return int(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "33fbb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(repo_names, repo_usernames, stars, repo_urls, topic_title_value):\n",
    "    \n",
    "    all_repo_dict = {'Repository Name': repo_names,\n",
    "                     'Repository Username': repo_usernames, \n",
    "                     'Stars Received': stars, \n",
    "                     'Repository URL': repo_urls}\n",
    "    \n",
    "    all_repo_df = pd.DataFrame(all_repo_dict)\n",
    "    \n",
    "    \n",
    "    print(topic_title_value)\n",
    "    print(all_repo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ecd99475",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Under the get_all_repo function we are looping through each topic under\n",
    "topic_urls to get the top 20 repository details under each topic.\n",
    "\n",
    "Under get_all_repo() we extract the following repository details for all the github-topics\n",
    "1. Repository Name\n",
    "2. Repository Username\n",
    "3. No. of stars repository received\n",
    "4. Repository URL\n",
    "\n",
    "'''\n",
    "\n",
    "def get_all_repo(topic_urls, topic_titles):\n",
    "    \n",
    "    # To loop through each links under topic_urls\n",
    "    for i in range(0,1):\n",
    "        \n",
    "        # Sending GET request for topic_url\n",
    "        res = requests.get(topic_urls[i])\n",
    "        \n",
    "        if res.status_code != 200:\n",
    "            raise Exception(f'Request to {topic_urls[i]} failed with status code {res.status_code}')\n",
    "        \n",
    "        # To pasre each topic to extract top 20 repository under each topic\n",
    "        repo = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        # repo_tags contains info about repo name, username and repo URL. Lets extract them one by one \n",
    "        repo_tags = repo.find_all('h3', {'class': 'f3 color-fg-muted text-normal lh-condensed'})\n",
    "        \n",
    "        # To append all the repository names for each topic\n",
    "        repo_names = []\n",
    "        repo_usernames = []\n",
    "        repo_urls = []\n",
    "        stars = []\n",
    "        \n",
    "        for i in range(len(repo_tags)):\n",
    "            repo_names.append(repo_tags[i].find_all('a')[1].text.strip())\n",
    "        \n",
    "        for i in range(len(repo_tags)):\n",
    "            repo_usernames.append(repo_tags[i].find_all('a')[0].text.strip())\n",
    "        \n",
    "        for i in range(len(repo_tags)):\n",
    "            repo_urls.append(base_url + repo_tags[i].find_all('a')[1]['href'])\n",
    "            \n",
    "        # star_tags contains info about the stars repository received\n",
    "        star_tags = repo.find_all('span', {'class': 'Counter js-social-count'})\n",
    "        \n",
    "        for i in range(len(star_tags)):\n",
    "            temp = get_star_num(star_tags[i].text)\n",
    "            stars.append(temp)\n",
    "        \n",
    "        create_csv(repo_names, repo_usernames, stars, repo_urls, topic_titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7d6aacbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Repository Name Repository Username  Stars Received  \\\n",
      "0                        three.js              mrdoob           90600   \n",
      "1               react-three-fiber              pmndrs           22100   \n",
      "2                          libgdx              libgdx           21300   \n",
      "3                      Babylon.js           BabylonJS           19800   \n",
      "4                    tinyrenderer               ssloy           16600   \n",
      "5                          aframe            aframevr           15200   \n",
      "6   3d-game-shaders-for-beginners             lettier           14900   \n",
      "7                         FreeCAD             FreeCAD           13700   \n",
      "8                          cesium            CesiumGS           10200   \n",
      "9                            zdog           metafizzy            9700   \n",
      "10            3D-Machine-Learning         timzhang642            8800   \n",
      "11                         Open3D             isl-org            8400   \n",
      "12                        blender             blender            8100   \n",
      "13             SpaceshipGenerator        a1studmuffin            7400   \n",
      "14                     BlenderGIS             domlysz            6200   \n",
      "15                          Fyrox         FyroxEngine            6000   \n",
      "16                       openscad            openscad            5500   \n",
      "17                   model-viewer              google            5500   \n",
      "18                       spritejs            spritejs            5200   \n",
      "19                 webglstudio.js             jagenjo            4900   \n",
      "\n",
      "                                       Repository URL  \n",
      "0                  https://github.com/mrdoob/three.js  \n",
      "1         https://github.com/pmndrs/react-three-fiber  \n",
      "2                    https://github.com/libgdx/libgdx  \n",
      "3             https://github.com/BabylonJS/Babylon.js  \n",
      "4               https://github.com/ssloy/tinyrenderer  \n",
      "5                  https://github.com/aframevr/aframe  \n",
      "6    https://github.com/lettier/3d-game-shaders-fo...  \n",
      "7                  https://github.com/FreeCAD/FreeCAD  \n",
      "8                  https://github.com/CesiumGS/cesium  \n",
      "9                   https://github.com/metafizzy/zdog  \n",
      "10   https://github.com/timzhang642/3D-Machine-Lea...  \n",
      "11                  https://github.com/isl-org/Open3D  \n",
      "12                 https://github.com/blender/blender  \n",
      "13   https://github.com/a1studmuffin/SpaceshipGene...  \n",
      "14              https://github.com/domlysz/BlenderGIS  \n",
      "15               https://github.com/FyroxEngine/Fyrox  \n",
      "16               https://github.com/openscad/openscad  \n",
      "17             https://github.com/google/model-viewer  \n",
      "18               https://github.com/spritejs/spritejs  \n",
      "19          https://github.com/jagenjo/webglstudio.js  \n",
      "C\n"
     ]
    }
   ],
   "source": [
    "get_all_repo(topic_urls, topic_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff591d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "common"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
