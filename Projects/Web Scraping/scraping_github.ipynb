{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7889417",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories For Trending Github Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9048f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74947aa1",
   "metadata": {},
   "source": [
    "## Project Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647c175",
   "metadata": {},
   "source": [
    "- In this project we are going to scrape https://github.com/topics\n",
    "- We are going to extract the trending github topics and for each topic we will extract the topic title, topic description and topic URL\n",
    "- For each topic we will extract 20 top repositories \n",
    "- For each repositories, we'll grab the repository name, its username, no. of stars the repository received, and the  repository URL\n",
    "- For each topic we'll create a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d858fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = ' https://github.com'\n",
    "main_url = 'https://github.com/topics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591f9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main objective of this get_all_topics function is to parse through all the pages of github_topics and return 3 things\n",
    "1. Topic_titles\n",
    "2. Topic_description\n",
    "3. Topic_urls\n",
    "'''\n",
    "\n",
    "def get_all_topics(main_url):\n",
    "    \n",
    "    # There are a total of 6 github-topics pages. Hence variable i is used to parse all 6 pages.\n",
    "    i = 1\n",
    "    \n",
    "    topic_titles = []\n",
    "    topic_description = []\n",
    "    topic_urls = []\n",
    "    \n",
    "    for i in range(1,7):\n",
    "        filters = {'page': i}\n",
    "        \n",
    "        # Sending the GET request to github-topics webpage\n",
    "        response = requests.get(main_url, params=filters)\n",
    "        \n",
    "        # If GET request not made successfully then terminate the code else continue\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f'Content did not get loaded for page no. {i}')\n",
    "        \n",
    "        # Doc has the content of the page from which we will extract information\n",
    "        \n",
    "        doc = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        '''\n",
    "        Now we need to extract 3 things here. Topic title, Topic description and Topic url.\n",
    "        '''\n",
    "        \n",
    "        # To extract the title of each github topic\n",
    "        topic_title_tags = doc.find_all('p', {'class': 'f3 lh-condensed mb-0 mt-1 Link--primary'})\n",
    "        \n",
    "        # Extracting the text of topic title and appending it in the topic_titles list created.\n",
    "        for tag in topic_title_tags:\n",
    "            topic_titles.append(tag.text)\n",
    "        \n",
    "        # To extract the description of each github topic\n",
    "        topic_desc_tags = doc.find_all('p', {'class': 'f5 color-fg-muted mb-0 mt-1'})\n",
    "\n",
    "        # Extracting the text of topic description and appending it in the topic_description list created.\n",
    "        for tag in topic_desc_tags:\n",
    "            topic_description.append(tag.text.strip())\n",
    "        \n",
    "        # To extract the link of each github topic\n",
    "        topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "        \n",
    "        # Extracting the link of each topic and appending it in the topic_urls list created.\n",
    "        for tag in topic_link_tags:\n",
    "            topic_urls.append(base_url + tag['href'])\n",
    "            \n",
    "    return topic_titles, topic_description, topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f56067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the get_all_topics function\n",
    "topic_titles, topic_description, topic_urls = get_all_topics(main_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b8269f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D',\n",
       " 'Ajax',\n",
       " 'Algorithm',\n",
       " 'Amp',\n",
       " 'Android',\n",
       " 'Angular',\n",
       " 'Ansible',\n",
       " 'API',\n",
       " 'Arduino',\n",
       " 'ASP.NET',\n",
       " 'Atom',\n",
       " 'Awesome Lists',\n",
       " 'Amazon Web Services',\n",
       " 'Azure',\n",
       " 'Babel',\n",
       " 'Bash',\n",
       " 'Bitcoin',\n",
       " 'Bootstrap',\n",
       " 'Bot',\n",
       " 'C',\n",
       " 'Chrome',\n",
       " 'Chrome extension',\n",
       " 'Command line interface',\n",
       " 'Clojure',\n",
       " 'Code quality',\n",
       " 'Code review',\n",
       " 'Compiler',\n",
       " 'Continuous integration',\n",
       " 'COVID-19',\n",
       " 'C++',\n",
       " 'Cryptocurrency',\n",
       " 'Crystal',\n",
       " 'C#',\n",
       " 'CSS',\n",
       " 'Data structures',\n",
       " 'Data visualization',\n",
       " 'Database',\n",
       " 'Deep learning',\n",
       " 'Dependency management',\n",
       " 'Deployment',\n",
       " 'Django',\n",
       " 'Docker',\n",
       " 'Documentation',\n",
       " '.NET',\n",
       " 'Electron',\n",
       " 'Elixir',\n",
       " 'Emacs',\n",
       " 'Ember',\n",
       " 'Emoji',\n",
       " 'Emulator',\n",
       " 'ESLint',\n",
       " 'Ethereum',\n",
       " 'Express',\n",
       " 'Firebase',\n",
       " 'Firefox',\n",
       " 'Flask',\n",
       " 'Font',\n",
       " 'Framework',\n",
       " 'Front end',\n",
       " 'Game engine',\n",
       " 'Git',\n",
       " 'GitHub API',\n",
       " 'Go',\n",
       " 'Google',\n",
       " 'Gradle',\n",
       " 'GraphQL',\n",
       " 'Gulp',\n",
       " 'Hacktoberfest',\n",
       " 'Haskell',\n",
       " 'Homebrew',\n",
       " 'Homebridge',\n",
       " 'HTML',\n",
       " 'HTTP',\n",
       " 'Icon font',\n",
       " 'iOS',\n",
       " 'IPFS',\n",
       " 'Java',\n",
       " 'JavaScript',\n",
       " 'Jekyll',\n",
       " 'jQuery',\n",
       " 'JSON',\n",
       " 'The Julia Language',\n",
       " 'Jupyter Notebook',\n",
       " 'Koa',\n",
       " 'Kotlin',\n",
       " 'Kubernetes',\n",
       " 'Laravel',\n",
       " 'LaTeX',\n",
       " 'Library',\n",
       " 'Linux',\n",
       " 'Localization',\n",
       " 'Lua',\n",
       " 'Machine learning',\n",
       " 'macOS',\n",
       " 'Markdown',\n",
       " 'Mastodon',\n",
       " 'Material Design',\n",
       " 'MATLAB',\n",
       " 'Maven',\n",
       " 'Minecraft',\n",
       " 'Mobile',\n",
       " 'Monero',\n",
       " 'MongoDB',\n",
       " 'Mongoose',\n",
       " 'Monitoring',\n",
       " 'MvvmCross',\n",
       " 'MySQL',\n",
       " 'NativeScript',\n",
       " 'Nim',\n",
       " 'Natural language processing',\n",
       " 'Node.js',\n",
       " 'NoSQL',\n",
       " 'npm',\n",
       " 'Objective-C',\n",
       " 'OpenGL',\n",
       " 'Operating system',\n",
       " 'P2P',\n",
       " 'Package manager',\n",
       " 'Parsing',\n",
       " 'Perl',\n",
       " 'Phaser',\n",
       " 'PHP',\n",
       " 'PICO-8',\n",
       " 'Pixel Art',\n",
       " 'PostgreSQL',\n",
       " 'Project management',\n",
       " 'Publishing',\n",
       " 'PWA',\n",
       " 'Python',\n",
       " 'Qt',\n",
       " 'R',\n",
       " 'Rails',\n",
       " 'Raspberry Pi',\n",
       " 'Ratchet',\n",
       " 'React',\n",
       " 'React Native',\n",
       " 'ReactiveUI',\n",
       " 'Redux',\n",
       " 'REST API',\n",
       " 'Ruby',\n",
       " 'Rust',\n",
       " 'Sass',\n",
       " 'Scala',\n",
       " 'scikit-learn',\n",
       " 'Software-defined networking',\n",
       " 'Security',\n",
       " 'Server',\n",
       " 'Serverless',\n",
       " 'Shell',\n",
       " 'Sketch',\n",
       " 'SpaceVim',\n",
       " 'Spring Boot',\n",
       " 'SQL',\n",
       " 'Storybook',\n",
       " 'Support',\n",
       " 'Swift',\n",
       " 'Symfony',\n",
       " 'Telegram',\n",
       " 'Tensorflow',\n",
       " 'Terminal',\n",
       " 'Terraform',\n",
       " 'Testing',\n",
       " 'Twitter',\n",
       " 'TypeScript',\n",
       " 'Ubuntu',\n",
       " 'Unity',\n",
       " 'Unreal Engine',\n",
       " 'Vagrant',\n",
       " 'Vim',\n",
       " 'Virtual reality',\n",
       " 'Vue.js',\n",
       " 'Wagtail',\n",
       " 'Web Components',\n",
       " 'Web app',\n",
       " 'Webpack',\n",
       " 'Windows',\n",
       " 'WordPlate',\n",
       " 'WordPress',\n",
       " 'Xamarin',\n",
       " 'XML']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a0b7df",
   "metadata": {},
   "source": [
    "### Creating a CSV file to store all three of the github-topics information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fe42fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binding the 3 lists on topics into a dictionary\n",
    "all_topics_dict = {'title': topic_titles, 'description': topic_description, 'url': topic_urls}\n",
    "\n",
    "# Converting the dictionary into a dataframe\n",
    "all_topics_df = pd.DataFrame(all_topics_dict)\n",
    "\n",
    "all_topics_df.to_csv('all_topics.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ae89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(topic_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4cd964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f7e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_tags = repo.find_all('h3', {'class': 'f3 color-fg-muted text-normal lh-condensed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26bca0f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf7f1158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mrdoob/three.js\n"
     ]
    }
   ],
   "source": [
    "print(repo_tags[0].find_all('a')[1]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69315e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_tags = repo.find_all('span', {'class': 'Counter js-social-count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "251ad240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'90.6k'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_tags[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a9ec4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_star_num(str):\n",
    "    if str[-1] == 'k':\n",
    "        return int(float(str[:-1])*1000)\n",
    "    return int(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "babf6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(repo_names, repo_usernames, stars, repo_urls, topic_title_value):\n",
    "    \n",
    "    all_repo_dict = {'Repository Name': repo_names,\n",
    "                     'Repository Username': repo_usernames, \n",
    "                     'Stars Received': stars, \n",
    "                     'Repository URL': repo_urls}\n",
    "    \n",
    "    all_repo_df = pd.DataFrame(all_repo_dict)\n",
    "    \n",
    "    all_repo_df.to_csv(f'{topic_title_value}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fbeb25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Under the get_all_repo function we are looping through each topic under\n",
    "topic_urls to get the top 20 repository details under each topic.\n",
    "\n",
    "Under get_all_repo() we extract the following repository details for all the github-topics\n",
    "1. Repository Name\n",
    "2. Repository Username\n",
    "3. No. of stars repository received\n",
    "4. Repository URL\n",
    "\n",
    "'''\n",
    "\n",
    "def get_all_repo(topic_urls, topic_titles):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # To loop through each links under topic_urls\n",
    "        for i in range(len(topic_urls)):\n",
    "        \n",
    "            topic_title_value = topic_titles[i]\n",
    "        \n",
    "            # Sending GET request for topic_url\n",
    "            res = requests.get(topic_urls[i])\n",
    "        \n",
    "            if res.status_code != 200:\n",
    "                raise Exception(f'Request to {topic_urls[i]} failed with status code {res.status_code}')\n",
    "        \n",
    "            # To pasre each topic to extract top 20 repository under each topic\n",
    "            repo = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "            # repo_tags contains info about repo name, username and repo URL. Lets extract them one by one \n",
    "            repo_tags = repo.find_all('h3', {'class': 'f3 color-fg-muted text-normal lh-condensed'})\n",
    "        \n",
    "            # To append all the repository names for each topic\n",
    "            repo_names = []\n",
    "            repo_usernames = []\n",
    "            repo_urls = []\n",
    "            stars = []\n",
    "        \n",
    "            for i in range(len(repo_tags)):\n",
    "                repo_names.append(repo_tags[i].find_all('a')[1].text.strip())\n",
    "        \n",
    "            for i in range(len(repo_tags)):\n",
    "                repo_usernames.append(repo_tags[i].find_all('a')[0].text.strip())\n",
    "        \n",
    "            for i in range(len(repo_tags)):\n",
    "                repo_urls.append(base_url + repo_tags[i].find_all('a')[1]['href'])\n",
    "            \n",
    "            # star_tags contains info about the stars repository received\n",
    "            star_tags = repo.find_all('span', {'class': 'Counter js-social-count'})\n",
    "        \n",
    "            for i in range(len(star_tags)):\n",
    "                temp = get_star_num(star_tags[i].text)\n",
    "                stars.append(temp)\n",
    "                \n",
    "            create_csv(repo_names, repo_usernames, stars, repo_urls, topic_title_value)\n",
    "        \n",
    "            repo_names = []\n",
    "            repo_usernames = []\n",
    "            repo_urls = []\n",
    "            stars = []\n",
    "        \n",
    "    except:\n",
    "        print(\"Connection refused by the server..\")\n",
    "        print(\"Let me sleep for 5 seconds\")\n",
    "        print(\"ZZzzzz...\")\n",
    "        time.sleep(5)\n",
    "        print(\"Was a nice sleep, now let me continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "05842e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_repo(topic_urls, topic_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099ed8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "common"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
